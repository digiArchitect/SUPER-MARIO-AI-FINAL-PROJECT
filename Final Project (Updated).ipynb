{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c96624a7-0092-4fb6-a237-093bfba745a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.26.4 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00dcbc3a-9183-42d0-9df5-a1201e1497f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.1.0 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: torchvision==0.16.0 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (0.16.0)\n",
      "Requirement already satisfied: torchaudio==2.1.0 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch==2.1.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch==2.1.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch==2.1.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch==2.1.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch==2.1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch==2.1.0) (2024.12.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from torchvision==0.16.0) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from torchvision==0.16.0) (2.32.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from torchvision==0.16.0) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from jinja2->torch==2.1.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->torchvision==0.16.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->torchvision==0.16.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->torchvision==0.16.0) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->torchvision==0.16.0) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from sympy->torch==2.1.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fc48f29-804e-41bc-9592-32fdd0bb66c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym-super-mario-bros==7.4.0 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (7.4.0)\n",
      "Requirement already satisfied: nes-py>=8.1.4 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from gym-super-mario-bros==7.4.0) (8.2.1)\n",
      "Requirement already satisfied: gym>=0.17.2 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.26.4)\n",
      "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.5.21)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (4.66.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (3.1.1)\n",
      "Requirement already satisfied: importlib_metadata>=4.8.0 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (8.5.0)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.0.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm>=4.48.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.4.6)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from importlib_metadata>=4.8.0->gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (3.21.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym-super-mario-bros==7.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff207443-6cb3-4122-94c7-2c6e9f38df0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensordict==0.3.0 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: torch>=2.1.0 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensordict==0.3.0) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensordict==0.3.0) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensordict==0.3.0) (3.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=2.1.0->tensordict==0.3.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=2.1.0->tensordict==0.3.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=2.1.0->tensordict==0.3.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=2.1.0->tensordict==0.3.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=2.1.0->tensordict==0.3.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=2.1.0->tensordict==0.3.0) (2024.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from jinja2->torch>=2.1.0->tensordict==0.3.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from sympy->torch>=2.1.0->tensordict==0.3.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensordict==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df8b48f9-ca7d-4319-a257-d265db7c7875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchrl==0.3.0 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: torch>=2.1.0 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from torchrl==0.3.0) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from torchrl==0.3.0) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from torchrl==0.3.0) (24.2)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from torchrl==0.3.0) (3.1.1)\n",
      "Requirement already satisfied: tensordict>=0.3.0 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from torchrl==0.3.0) (0.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=2.1.0->torchrl==0.3.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=2.1.0->torchrl==0.3.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=2.1.0->torchrl==0.3.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=2.1.0->torchrl==0.3.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=2.1.0->torchrl==0.3.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=2.1.0->torchrl==0.3.0) (2024.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from jinja2->torch>=2.1.0->torchrl==0.3.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from sympy->torch>=2.1.0->torchrl==0.3.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchrl==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0752609b-879c-4276-a14e-dfdc880ff8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages\\torchrl\\data\\replay_buffers\\samplers.py:37: UserWarning: Failed to import torchrl C++ binaries. Some modules (eg, prioritized replay buffers) may not work with your installation. If you installed TorchRL from PyPI, please report the bug on TorchRL github. If you installed TorchRL locally and/or in development mode, check that you have all the required compiling packages.\n",
      "  warnings.warn(EXTENSION_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0329000-3ad1-4290-9c30-e75ad77e8bf4",
   "metadata": {},
   "source": [
    "Initializing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad6b51b8-6285-4684-bceb-4a6956181dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages\\gym\\envs\\registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: [['NOOP'], ['right'], ['right', 'A'], ['right', 'B'], ['right', 'A', 'B'], ['A'], ['left']]\n",
      "(240, 256, 3),\n",
      " 0.0,\n",
      " False,\n",
      " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "if gym.__version__ < '0.26':\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n",
    "else:\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='human', apply_api_compatibility=True)\n",
    "\n",
    "# env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "# env = JoypadSpace(env, [[], [\"right\"], [\"right\", \"A\"]])\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "print(\"Action space:\", SIMPLE_MOVEMENT)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, trunc, info = env.step(action=0)\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dc8298-c334-4aff-8fa2-a2035aa1798a",
   "metadata": {},
   "source": [
    "Using wrappers to preprocess environment data to give our agent only information it needs: making the structure of the environment a 3D array: [4, 84, 84]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "609fca43-4c7c-466d-ab1f-94279f56a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n",
    "\n",
    "\n",
    "# env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "if gym.__version__ < '0.26':\n",
    "    env = FrameStack(env, num_stack=4, new_step_api=True)\n",
    "else:\n",
    "    env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38a3028-e5e6-46e6-ac39-d80a6609cc2f",
   "metadata": {},
   "source": [
    "#### Agent: Mario  \n",
    "Goals  \n",
    "- Makes optimal action policy based on current state  \n",
    "- Learns a better action policy over time\n",
    "- Q learning agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797b22e8-a9c5-416b-a3f7-ffcace617899",
   "metadata": {},
   "source": [
    "#### Scripted Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d6c90fd-7a14-4017-b66c-79796a68fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scripted_expert_policy(obs, step_count):\n",
    "    if 0 <= (step_count % 20) < 17:  \n",
    "        return 2  # ['right', 'A']\n",
    "    return 1  # ['right']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35a40302-060c-4f68-92d3-6d1eb9525e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_scripted_expert_data(env, save_path=\"scripted_demo.pkl\", episodes=5):\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "\n",
    "    MAX_STEPS = 1000\n",
    "\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        for ep in range(episodes):\n",
    "            state = env.reset()[0]\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            print(f\"Episode {ep+1}\")\n",
    "\n",
    "            while not done and step_count < MAX_STEPS:\n",
    "\n",
    "                action = scripted_expert_policy(state, step_count)\n",
    "                pickle.dump((np.array(state), action), f)\n",
    "\n",
    "                state, _, done, _, _ = env.step(action)\n",
    "                step_count += 1\n",
    "\n",
    "            print(f\"Episode {ep+1} done in {step_count} steps.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "060a481a-4f4f-416b-a1c4-44c10f7767b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "Episode 1 done in 1000 steps.\n",
      "Episode 2\n",
      "Episode 2 done in 1000 steps.\n",
      "Episode 3\n",
      "Episode 3 done in 1000 steps.\n",
      "Episode 4\n",
      "Episode 4 done in 1000 steps.\n",
      "Episode 5\n",
      "Episode 5 done in 1000 steps.\n"
     ]
    }
   ],
   "source": [
    "collect_scripted_expert_data(env, save_path=\"scripted_demo.pk1\", episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20382e6e-524d-466f-9cd3-b9f4e7e37fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameSequenceDataset(Dataset):\n",
    "    def __init__(self, path, seq_len=8):\n",
    "        import pickle\n",
    "\n",
    "        frames = []\n",
    "        actions = []\n",
    "\n",
    "        with open(path, \"rb\") as f:\n",
    "            while True:\n",
    "                try:\n",
    "                    state, action = pickle.load(f)\n",
    "                    frames.append(torch.tensor(state, dtype=torch.float32) / 255.0)\n",
    "                    actions.append(action)\n",
    "                except EOFError:\n",
    "                    break\n",
    "\n",
    "        # sequences\n",
    "        self.sequences = []\n",
    "        self.labels = []\n",
    "        for i in range(len(frames) - seq_len):\n",
    "            sequence = torch.stack(frames[i:i+seq_len]).unsqueeze(1)  \n",
    "            label = actions[i + seq_len - 1]\n",
    "            self.sequences.append(sequence)\n",
    "            self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], torch.tensor(self.labels[idx], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99e86367-bb02-481a-8269-7b55936ff4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FrameSequenceDataset(\"scripted_demo.pk1\", seq_len=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49f4984d-d468-4293-9f28-52a52de1529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_scripted_agent(env, episodes=5, render=False):\n",
    "    rewards = []\n",
    "    distances = []\n",
    "    steps_survived = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            action = scripted_expert_policy(state, step_count)\n",
    "            state, reward, done, _, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            step_count += 1\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        distances.append(info.get('x_pos', 0))\n",
    "        steps_survived.append(step_count)\n",
    "\n",
    "        print(f\"[Scripted] Ep {ep+1}: Reward={total_reward}, Distance={distances[-1]}, Steps={step_count}\")\n",
    "\n",
    "    print(f\"[Scripted] Avg Reward: {sum(rewards)/len(rewards):.2f}\")\n",
    "    print(f\"[Scripted] Avg Distance: {sum(distances)/len(distances):.2f}\")\n",
    "    print(f\"[Scripted] Avg Steps Survived: {sum(steps_survived)/len(steps_survived):.2f}\")\n",
    "    \n",
    "    return rewards, distances, steps_survived\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab44f4db-f6af-46a1-b090-748af233b28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:272: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Scripted] Ep 1: Reward=3005.0, Distance=3161, Steps=2334\n",
      "[Scripted] Avg Reward: 3005.00\n",
      "[Scripted] Avg Distance: 3161.00\n",
      "[Scripted] Avg Steps Survived: 2334.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([3005.0], [3161], [2334])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_scripted_agent(env, episodes=1, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774c2a69-d1b6-4d16-b39b-87e146758dce",
   "metadata": {},
   "source": [
    "#### Imitation Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd581b60-5b26-4a2d-b46d-166395806f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "class ExpertDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.csv_files = list(self.root_dir.glob(\"*.csv\"))\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        \n",
    "        for csv_file in self.csv_files:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            for _, row in df.iterrows():\n",
    "                img_path = self.root_dir / row['state_path']\n",
    "                self.data.append((img_path, row['action']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, action = self.data[idx]\n",
    "        image = Image.open(img_path).convert('L')  \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, torch.tensor(action, dtype=torch.long)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f35697e8-419e-4485-aff8-cba9b6bf5335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keyboard in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (0.13.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install keyboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f1b647d-f43f-4692-b8b2-ee4a42cac59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygame in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: gym-super-mario-bros==7.4.0 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (7.4.0)\n",
      "Requirement already satisfied: nes-py>=8.1.4 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from gym-super-mario-bros==7.4.0) (8.2.1)\n",
      "Requirement already satisfied: gym>=0.17.2 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.26.4)\n",
      "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.5.21)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (4.66.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (3.1.1)\n",
      "Requirement already satisfied: importlib_metadata>=4.8.0 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (8.5.0)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.0.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm>=4.48.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.4.6)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages (from importlib_metadata>=4.8.0->gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (3.21.0)\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def collect_expert_demos(save_dir=\"expert_demos\"):\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v3\", \n",
    "                                   render_mode='human',\n",
    "                                   apply_api_compatibility=True)\n",
    "    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "    \n",
    "    pygame.init()\n",
    "    screen = pygame.display.set_mode((256, 240))\n",
    "    \n",
    "    save_path = Path(save_dir)\n",
    "    save_path.mkdir(exist_ok=True)\n",
    "    demo_num = len(list(save_path.glob(\"*.csv\")))\n",
    "    csv_path = save_path / f\"demo_{demo_num}.csv\"\n",
    "    df = pd.DataFrame(columns=[\"state_path\", \"action\"])\n",
    "\n",
    "    state = env.reset()\n",
    "    clock = pygame.time.Clock()\n",
    "    running = True\n",
    "    action = 0\n",
    "    \n",
    "    print(\"Use Arrow Keys + Space to control!\")\n",
    "    print(\"→: Right | ←: Left | ↑: Jump | Q: Quit\")\n",
    "\n",
    "    while running:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "        \n",
    "        keys = pygame.key.get_pressed()\n",
    "        \n",
    "        if keys[pygame.K_RIGHT] and keys[pygame.K_UP]:\n",
    "            action = 2  # Jump right\n",
    "        elif keys[pygame.K_RIGHT]:\n",
    "            action = 1  # Right\n",
    "        elif keys[pygame.K_LEFT]:\n",
    "            action = 6  # Left\n",
    "        elif keys[pygame.K_UP]:\n",
    "            action = 5  # Jump\n",
    "        elif keys[pygame.K_q]:\n",
    "            break\n",
    "        else:\n",
    "            action = 0\n",
    "            \n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "        \n",
    "        img_path = save_path / f\"frame_{demo_num}_{len(df)}.png\"\n",
    "        Image.fromarray(next_state).save(img_path)\n",
    "        df.loc[len(df)] = [img_path.name, action]\n",
    "        \n",
    "        if done:\n",
    "            state = env.reset()\n",
    "        \n",
    "        clock.tick(60)  \n",
    "\n",
    "    pygame.quit()\n",
    "    env.close()\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved {len(df)} frames!\")\n",
    "\n",
    "!pip install pygame gym-super-mario-bros==7.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e31dc4ed-3679-4a5e-b446-ee8764030931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages\\gym\\envs\\registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Arrow Keys + Space to control!\n",
      "→: Right | ←: Left | ↑: Jump | Q: Quit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 18369 frames!\n"
     ]
    }
   ],
   "source": [
    "collect_expert_demos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2faf05d6-6c61-4bf8-8d69-be081bc08c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "\n",
    "class MarioNet(torch.nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, 8, stride=4),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, 4, stride=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, 3, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(3136, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def train():\n",
    "    BATCH_SIZE = 32\n",
    "    LR = 0.00025\n",
    "    EPOCHS = 10\n",
    "    \n",
    "    transform = T.Compose([\n",
    "        T.Grayscale(),\n",
    "        T.Resize((84, 84)),\n",
    "        T.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    dataset = ExpertDataset(\"expert_demos\", transform=transform)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # initialize model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MarioNet(input_shape=(1, 84, 84), num_actions=env.action_space.n)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # train\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for states, actions in train_loader:\n",
    "            states = states.to(device)\n",
    "            actions = actions.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(states)\n",
    "            loss = criterion(outputs, actions)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for states, actions in val_loader:\n",
    "                states = states.to(device)\n",
    "                actions = actions.to(device)\n",
    "                \n",
    "                outputs = model(states)\n",
    "                loss = criterion(outputs, actions)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == actions).sum().item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "        print(f\"Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "        print(f\"Val Acc: {correct/len(val_set):.4f}\\n\")\n",
    "    \n",
    "    torch.save(model.state_dict(), \"mario_net.pt\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f9ebfe7-7a15-4d85-928a-90c92c425e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, episodes=5):\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v3\", \n",
    "                                   render_mode='human',\n",
    "                                   apply_api_compatibility=True)\n",
    "    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "    env = GrayScaleObservation(env)\n",
    "    env = ResizeObservation(env, shape=84)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # metrics\n",
    "    total_rewards = []\n",
    "    total_distances = []\n",
    "    steps_survived = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        raw_state = env.reset()\n",
    "        state = raw_state[0] if isinstance(raw_state, tuple) else raw_state\n",
    "        \n",
    "        state_tensor = torch.tensor(state).float()\n",
    "        if len(state_tensor.shape) == 2:  \n",
    "            state_tensor = state_tensor.unsqueeze(0).unsqueeze(0)  \n",
    "        elif len(state_tensor.shape) == 3:\n",
    "            state_tensor = state_tensor.unsqueeze(0)  \n",
    "        state_tensor = state_tensor.to(device)\n",
    "        \n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "        max_x = 0\n",
    "        \n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                action = model(state_tensor).argmax().item()\n",
    "            \n",
    "            # step response\n",
    "            step_response = env.step(action)\n",
    "            next_state = step_response[0]\n",
    "            reward = step_response[1]\n",
    "            done = step_response[2]\n",
    "            info = step_response[4]  \n",
    "            \n",
    "            # next state\n",
    "            next_state_tensor = torch.tensor(next_state).float()\n",
    "            if len(next_state_tensor.shape) == 2:\n",
    "                next_state_tensor = next_state_tensor.unsqueeze(0).unsqueeze(0)\n",
    "            elif len(next_state_tensor.shape) == 3:\n",
    "                next_state_tensor = next_state_tensor.unsqueeze(0)\n",
    "            next_state_tensor = next_state_tensor.to(device)\n",
    "            \n",
    "            # metrics\n",
    "            episode_reward += reward\n",
    "            episode_steps += 1\n",
    "            max_x = max(max_x, info['x_pos'])\n",
    "            env.render()\n",
    "            \n",
    "            state_tensor = next_state_tensor\n",
    "        \n",
    "        # results\n",
    "        total_rewards.append(episode_reward)\n",
    "        total_distances.append(max_x)\n",
    "        steps_survived.append(episode_steps)\n",
    "        \n",
    "        print(f\"Episode {episode+1}\")\n",
    "        print(f\"  Reward: {episode_reward} | Distance: {max_x} | Steps: {episode_steps}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    avg_reward = sum(total_rewards)/len(total_rewards)\n",
    "    avg_distance = sum(total_distances)/len(total_distances)\n",
    "    avg_steps = sum(steps_survived)/len(steps_survived)\n",
    "    \n",
    "    print(\"\\nFinal Evaluation Metrics:\")\n",
    "    print(f\"[Imitation Agent] Avg Reward: {avg_reward:.2f}\")\n",
    "    print(f\"[Imitation Agent] Avg Distance: {avg_distance:.2f}\")\n",
    "    print(f\"[Imitation Agent] Avg Steps Survived: {avg_steps:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'reward': avg_reward,\n",
    "        'distance': avg_distance,\n",
    "        'steps': avg_steps\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "105857d7-03f8-431e-84ca-b4ab86aaeff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 1.0098\n",
      "Val Loss: 0.8786\n",
      "Val Acc: 0.6662\n",
      "\n",
      "Epoch 2/10\n",
      "Train Loss: 0.8199\n",
      "Val Loss: 0.7864\n",
      "Val Acc: 0.7070\n",
      "\n",
      "Epoch 3/10\n",
      "Train Loss: 0.7585\n",
      "Val Loss: 0.7408\n",
      "Val Acc: 0.7226\n",
      "\n",
      "Epoch 4/10\n",
      "Train Loss: 0.6940\n",
      "Val Loss: 0.6776\n",
      "Val Acc: 0.7361\n",
      "\n",
      "Epoch 5/10\n",
      "Train Loss: 0.6435\n",
      "Val Loss: 0.6292\n",
      "Val Acc: 0.7535\n",
      "\n",
      "Epoch 6/10\n",
      "Train Loss: 0.5969\n",
      "Val Loss: 0.5875\n",
      "Val Acc: 0.7753\n",
      "\n",
      "Epoch 7/10\n",
      "Train Loss: 0.5544\n",
      "Val Loss: 0.5494\n",
      "Val Acc: 0.7869\n",
      "\n",
      "Epoch 8/10\n",
      "Train Loss: 0.5235\n",
      "Val Loss: 0.5231\n",
      "Val Acc: 0.8034\n",
      "\n",
      "Epoch 9/10\n",
      "Train Loss: 0.4906\n",
      "Val Loss: 0.4939\n",
      "Val Acc: 0.8170\n",
      "\n",
      "Epoch 10/10\n",
      "Train Loss: 0.4613\n",
      "Val Loss: 0.4726\n",
      "Val Acc: 0.8245\n",
      "\n",
      "MarioNet(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (8): ReLU()\n",
      "    (9): Linear(in_features=512, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "trained_model = train()\n",
    "print(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ee9ad02-3ca9-4ec9-a7f9-cf7d409d60b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bkuzn\\AppData\\Local\\Temp\\ipykernel_25580\\3449933794.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(state).float()\n",
      "C:\\Users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "C:\\Users\\bkuzn\\AppData\\Local\\Temp\\ipykernel_25580\\3449933794.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_state_tensor = torch.tensor(next_state).float()\n",
      "C:\\Users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:272: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "  Reward: -415.0 | Distance: 40 | Steps: 9623\n",
      "Episode 2\n",
      "  Reward: -415.0 | Distance: 40 | Steps: 9623\n",
      "Episode 3\n",
      "  Reward: -415.0 | Distance: 40 | Steps: 9623\n",
      "Episode 4\n",
      "  Reward: -415.0 | Distance: 40 | Steps: 9623\n",
      "Episode 5\n",
      "  Reward: -415.0 | Distance: 40 | Steps: 9623\n",
      "\n",
      "Final Evaluation Metrics:\n",
      "[Imitation Agent] Avg Reward: -415.00\n",
      "[Imitation Agent] Avg Distance: 40.00\n",
      "[Imitation Agent] Avg Steps Survived: 9623.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'reward': -415.0, 'distance': 40.0, 'steps': 9623.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3103255d-f5c3-401b-a5d8-481f676b6929",
   "metadata": {},
   "source": [
    "##### Using CNN + LSTM approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54777d96-02a9-4e9e-85b4-f5690215f7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialExpertDataset(Dataset):\n",
    "    def __init__(self, expert_data):\n",
    "        self.sequences = []\n",
    "        self.labels = []\n",
    "\n",
    "        for state, action in expert_data:\n",
    "            full_stack = torch.tensor(np.array(state), dtype=torch.float32) / 255.0  \n",
    "            seq = full_stack.unsqueeze(1)  \n",
    "            self.sequences.append(seq)     \n",
    "            self.labels.append(action)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], torch.tensor(self.labels[idx], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cad38fbc-c906-47fc-be51-75da5740a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImitationCNNLSTM(nn.Module):\n",
    "    def __init__(self, num_actions=2, cnn_output_size=512, hidden_size=256, lstm_layers=1):\n",
    "        super(ImitationCNNLSTM, self).__init__()\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=8, stride=4),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, cnn_output_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=cnn_output_size, hidden_size=hidden_size, num_layers=lstm_layers, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: [batch_size, seq_len, 1, 84, 84]\n",
    "        \"\"\"\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B * T, C, H, W)        \n",
    "        features = self.cnn(x)             \n",
    "        features = features.view(B, T, -1) \n",
    "\n",
    "        lstm_out, _ = self.lstm(features) \n",
    "        final_output = lstm_out[:, -1, :] \n",
    "\n",
    "        return self.fc(final_output)      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b213356-70d5-4dc3-8b73-ff4ef783ec5d",
   "metadata": {},
   "source": [
    "##### Splitting into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35b35497-ea12-4b34-b7d8-bb590e42dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_data = torch.load(\"expert_demo.pt\")\n",
    "\n",
    "train_data, test_data = train_test_split(expert_data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = SequentialExpertDataset(train_data)\n",
    "test_dataset = SequentialExpertDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "289bdb86-0ae1-4545-9903-bd265c000cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_lstm = ImitationCNNLSTM(num_actions=2).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d418d803-19e5-4e67-b692-ab4cfcea27e6",
   "metadata": {},
   "source": [
    "##### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a7f6734c-6801-40c4-a4c7-a99deee7a8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [1/5], Loss: 0.0016\n",
      "Epoch [1] Completed - Average Loss: 0.0010\n",
      "Epoch [2/10], Batch [1/5], Loss: 0.0004\n",
      "Epoch [2] Completed - Average Loss: 0.0003\n",
      "Epoch [3/10], Batch [1/5], Loss: 0.0002\n",
      "Epoch [3] Completed - Average Loss: 0.0001\n",
      "Epoch [4/10], Batch [1/5], Loss: 0.0001\n",
      "Epoch [4] Completed - Average Loss: 0.0001\n",
      "Epoch [5/10], Batch [1/5], Loss: 0.0001\n",
      "Epoch [5] Completed - Average Loss: 0.0001\n",
      "Epoch [6/10], Batch [1/5], Loss: 0.0000\n",
      "Epoch [6] Completed - Average Loss: 0.0000\n",
      "Epoch [7/10], Batch [1/5], Loss: 0.0000\n",
      "Epoch [7] Completed - Average Loss: 0.0000\n",
      "Epoch [8/10], Batch [1/5], Loss: 0.0000\n",
      "Epoch [8] Completed - Average Loss: 0.0000\n",
      "Epoch [9/10], Batch [1/5], Loss: 0.0000\n",
      "Epoch [9] Completed - Average Loss: 0.0000\n",
      "Epoch [10/10], Batch [1/5], Loss: 0.0000\n",
      "Epoch [10] Completed - Average Loss: 0.0000\n",
      "Model saved as cnn_lstm_imitation_model.pth\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model_lstm.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (sequences, actions) in enumerate(train_loader):\n",
    "        sequences = sequences.to(device)  \n",
    "        actions = actions.to(device)      \n",
    "\n",
    "        # forward\n",
    "        logits = model(sequences)         \n",
    "        loss = loss_fn(logits, actions)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}] Completed - Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "torch.save(model_lstm.state_dict(), \"cnn_lstm_imitation_model.pth\")\n",
    "print(\"Model saved as cnn_lstm_imitation_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292b4263-3813-4575-a981-25b98de39be2",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbc5fce-4810-472b-b445-ba00106ae969",
   "metadata": {},
   "source": [
    "##### Evaluating accuracy of imitation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "413016e6-d568-407c-b20f-e740ec89d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "expert_data = torch.load(\"expert_demo.pt\")\n",
    "train_data, test_data = train_test_split(expert_data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = ExpertDataset(train_data)\n",
    "test_dataset = ExpertDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eb6a1ff3-ff17-45b1-aab4-a13c19473f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_imitation_model(model, env, episodes=5, device=\"cpu\", render=False):\n",
    "    model.eval()\n",
    "    rewards = []\n",
    "    distances = []\n",
    "    steps_survived = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            state_tensor = torch.tensor(np.array(state), dtype=torch.float32).unsqueeze(0).unsqueeze(2).to(device)\n",
    "            state_tensor = state_tensor / 255.0\n",
    "\n",
    "\n",
    "            # predicting the action\n",
    "            with torch.no_grad():\n",
    "                logits = model(state_tensor)\n",
    "                action = logits.argmax(dim=1).item()\n",
    "\n",
    "            state, reward, done, _, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            step_count += 1\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        distances.append(info.get('x_pos', 0))\n",
    "        steps_survived.append(step_count)\n",
    "\n",
    "        print(f\"Episode {ep+1}: Reward={total_reward}, Distance={distances[-1]}, Steps={step_count}\")\n",
    "\n",
    "    print(f\"Avg Reward: {sum(rewards)/len(rewards):.2f}\")\n",
    "    print(f\"Avg Distance: {sum(distances)/len(distances):.2f}\")\n",
    "    print(f\"Avg Steps Survived: {sum(steps_survived)/len(steps_survived):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2f175b5-b6c1-4faa-aedd-67639ba5e68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward=231.0, Distance=296, Steps=40\n",
      "Episode 2: Reward=231.0, Distance=296, Steps=40\n",
      "Episode 3: Reward=231.0, Distance=296, Steps=40\n",
      "Episode 4: Reward=231.0, Distance=296, Steps=40\n",
      "Episode 5: Reward=231.0, Distance=296, Steps=40\n",
      "Avg Reward: 231.00\n",
      "Avg Distance: 296.00\n",
      "Avg Steps Survived: 40.00\n"
     ]
    }
   ],
   "source": [
    "evaluate_imitation_model(model_lstm, env, episodes=5, device=device, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d618e5f-e72a-42e6-ad34-68a2622673d4",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e850376-5b39-4477-99a7-c65c309b69b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioDQN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(MarioDQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "num_actions = len(SIMPLE_MOVEMENT)  \n",
    "policy_net = MarioDQN(num_actions).to(device)\n",
    "target_net = MarioDQN(num_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "replay_buffer = deque(maxlen=10000)\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon_start = 1.0\n",
    "epsilon_min = 0.05\n",
    "epsilon_decay = 0.995\n",
    "learning_rate = 0.00025\n",
    "update_interval = 4\n",
    "target_update_interval = 1000  \n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def preprocess_state(state):\n",
    "    state = torch.tensor(np.array(state), dtype=torch.float32).to(device)\n",
    "    state = state.permute(2, 0, 1).unsqueeze(0) / 255.0 \n",
    "    return state\n",
    "\n",
    "def choose_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, num_actions-1) \n",
    "    state = preprocess_state(state)\n",
    "    with torch.no_grad():\n",
    "        return policy_net(state).argmax().item()\n",
    "\n",
    "def train_dqn():\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "    \n",
    "    batch = random.sample(replay_buffer, batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    \n",
    "    states = torch.stack([preprocess_state(s) for s in states]).squeeze(1)\n",
    "    next_states = torch.stack([preprocess_state(s) for s in next_states]).squeeze(1)\n",
    "    actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "    \n",
    "    current_q = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        next_actions = policy_net(next_states).argmax(dim=1)\n",
    "        next_q = target_net(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()\n",
    "        target = rewards + gamma * next_q * (1 - dones)\n",
    "    \n",
    "    # Compute  and update\n",
    "    loss = loss_fn(current_q, target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(policy_net.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "epsilon = epsilon_start\n",
    "episodes = 500\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    total_steps = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = choose_action(state, epsilon)\n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "        \n",
    "        reward = np.clip(reward, -1, 1)\n",
    "        \n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "        total_steps += 1\n",
    "        \n",
    "        if total_steps % update_interval == 0:\n",
    "            train_dqn()\n",
    "            \n",
    "        if total_steps % target_update_interval == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    \n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
