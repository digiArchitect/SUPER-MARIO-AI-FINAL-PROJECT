{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c96624a7-0092-4fb6-a237-093bfba745a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dcbc3a-9183-42d0-9df5-a1201e1497f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc48f29-804e-41bc-9592-32fdd0bb66c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym-super-mario-bros==7.4.0\n",
      "  Obtaining dependency information for gym-super-mario-bros==7.4.0 from https://files.pythonhosted.org/packages/0c/4f/0951ba1480c6f874a5e3b043f20d06fc1951cc03844e8bcfa6208291d712/gym_super_mario_bros-7.4.0-py3-none-any.whl.metadata\n",
      "  Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting nes-py>=8.1.4 (from gym-super-mario-bros==7.4.0)\n",
      "  Downloading nes_py-8.2.1.tar.gz (77 kB)\n",
      "     ---------------------------------------- 0.0/77.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/77.7 kB ? eta -:--:--\n",
      "     ------------------------- ------------ 51.2/77.7 kB 890.4 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 77.7/77.7 kB 1.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting gym>=0.17.2 (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0)\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "     ---------------------------------------- 0.0/721.7 kB ? eta -:--:--\n",
      "     ---- ---------------------------------- 81.9/721.7 kB 2.3 MB/s eta 0:00:01\n",
      "     ------- ------------------------------ 133.1/721.7 kB 1.3 MB/s eta 0:00:01\n",
      "     ------- ------------------------------ 133.1/721.7 kB 1.3 MB/s eta 0:00:01\n",
      "     ---------- --------------------------- 204.8/721.7 kB 1.2 MB/s eta 0:00:01\n",
      "     ------------ ------------------------- 245.8/721.7 kB 1.0 MB/s eta 0:00:01\n",
      "     ------------------ ------------------- 358.4/721.7 kB 1.2 MB/s eta 0:00:01\n",
      "     ---------------------- --------------- 419.8/721.7 kB 1.4 MB/s eta 0:00:01\n",
      "     ---------------------- --------------- 419.8/721.7 kB 1.4 MB/s eta 0:00:01\n",
      "     ---------------------- --------------- 430.1/721.7 kB 1.0 MB/s eta 0:00:01\n",
      "     ------------------------- ------------ 481.3/721.7 kB 1.0 MB/s eta 0:00:01\n",
      "     ------------------------------ ------- 573.4/721.7 kB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------- ----- 614.4/721.7 kB 1.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- -- 665.6/721.7 kB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 721.7/721.7 kB 1.1 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.26.4)\n",
      "Collecting pyglet<=1.5.21,>=1.4.0 (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0)\n",
      "  Obtaining dependency information for pyglet<=1.5.21,>=1.4.0 from https://files.pythonhosted.org/packages/0b/b7/7736d7638d91b354700dc9bae447728c514c4bc6ecb4c0f7e0cd9a390f20/pyglet-1.5.21-py3-none-any.whl.metadata\n",
      "  Downloading pyglet-1.5.21-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (4.65.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (2.2.1)\n",
      "Collecting gym_notices>=0.0.4 (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0)\n",
      "  Obtaining dependency information for gym_notices>=0.0.4 from https://files.pythonhosted.org/packages/25/26/d786c6bec30fe6110fd3d22c9a273a2a0e56c0b73b93e25ea1af5a53243b/gym_notices-0.0.8-py3-none-any.whl.metadata\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from tqdm>=4.48.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.4.6)\n",
      "Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl (199 kB)\n",
      "   ---------------------------------------- 0.0/199.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 199.1/199.1 kB 6.1 MB/s eta 0:00:00\n",
      "Downloading pyglet-1.5.21-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.5/1.1 MB 9.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 0.7/1.1 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.1/1.1 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.1/1.1 MB 7.1 MB/s eta 0:00:00\n",
      "Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Building wheels for collected packages: nes-py, gym\n",
      "  Building wheel for nes-py (setup.py): started\n",
      "  Building wheel for nes-py (setup.py): finished with status 'done'\n",
      "  Created wheel for nes-py: filename=nes_py-8.2.1-cp311-cp311-win_amd64.whl size=46132 sha256=3fe828435d3176007dc06e7b9b604d8ce3538a33d04467c7cb374bc9c560c3e6\n",
      "  Stored in directory: c:\\users\\zoey\\appdata\\local\\pip\\cache\\wheels\\be\\b4\\5a\\68b9155f1d2380af0e359c71efd4c70518555be4c2f577f1d3\n",
      "  Building wheel for gym (pyproject.toml): started\n",
      "  Building wheel for gym (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827740 sha256=f25d02d72a583f655aad310ff487cedb2578b799e0708577a1e739981114930d\n",
      "  Stored in directory: c:\\users\\zoey\\appdata\\local\\pip\\cache\\wheels\\1c\\77\\9e\\9af5470201a0b0543937933ee99ba884cd237d2faefe8f4d37\n",
      "Successfully built nes-py gym\n",
      "Installing collected packages: pyglet, gym_notices, gym, nes-py, gym-super-mario-bros\n",
      "Successfully installed gym-0.26.2 gym-super-mario-bros-7.4.0 gym_notices-0.0.8 nes-py-8.2.1 pyglet-1.5.21\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym-super-mario-bros==7.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff207443-6cb3-4122-94c7-2c6e9f38df0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensordict==0.3.0\n",
      "  Obtaining dependency information for tensordict==0.3.0 from https://files.pythonhosted.org/packages/2c/f6/04ff80bbca8cbd13fda3de681926ebc11fb669271c80728a2f7e8d666590/tensordict-0.3.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tensordict-0.3.0-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: torch>=2.1.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from tensordict==0.3.0) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from tensordict==0.3.0) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from tensordict==0.3.0) (2.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->tensordict==0.3.0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->tensordict==0.3.0) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->tensordict==0.3.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->tensordict==0.3.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->tensordict==0.3.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->tensordict==0.3.0) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.1.0->tensordict==0.3.0) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from sympy->torch>=2.1.0->tensordict==0.3.0) (1.3.0)\n",
      "Downloading tensordict-0.3.0-cp311-cp311-win_amd64.whl (264 kB)\n",
      "   ---------------------------------------- 0.0/264.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/264.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/264.9 kB ? eta -:--:--\n",
      "   ------ -------------------------------- 41.0/264.9 kB 487.6 kB/s eta 0:00:01\n",
      "   ------------------ --------------------- 122.9/264.9 kB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 204.8/264.9 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 264.9/264.9 kB 1.2 MB/s eta 0:00:00\n",
      "Installing collected packages: tensordict\n",
      "Successfully installed tensordict-0.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensordict==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b48f9-ca7d-4319-a257-d265db7c7875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchrl==0.3.0\n",
      "  Obtaining dependency information for torchrl==0.3.0 from https://files.pythonhosted.org/packages/d3/2e/5963f0c27b3223dea3ef03ccad1e50ace6c9d79bbfdaac299f79405e5bb5/torchrl-0.3.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading torchrl-0.3.0-cp311-cp311-win_amd64.whl.metadata (32 kB)\n",
      "Requirement already satisfied: torch>=2.1.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torchrl==0.3.0) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torchrl==0.3.0) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torchrl==0.3.0) (23.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torchrl==0.3.0) (2.2.1)\n",
      "Requirement already satisfied: tensordict>=0.3.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torchrl==0.3.0) (0.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->torchrl==0.3.0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->torchrl==0.3.0) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->torchrl==0.3.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->torchrl==0.3.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->torchrl==0.3.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->torchrl==0.3.0) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.1.0->torchrl==0.3.0) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from sympy->torch>=2.1.0->torchrl==0.3.0) (1.3.0)\n",
      "Downloading torchrl-0.3.0-cp311-cp311-win_amd64.whl (833 kB)\n",
      "   ---------------------------------------- 0.0/833.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 10.2/833.2 kB ? eta -:--:--\n",
      "   - ------------------------------------- 30.7/833.2 kB 435.7 kB/s eta 0:00:02\n",
      "   --- ----------------------------------- 81.9/833.2 kB 919.0 kB/s eta 0:00:01\n",
      "   -------- ------------------------------- 174.1/833.2 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 225.3/833.2 kB 1.3 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 286.7/833.2 kB 1.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 399.4/833.2 kB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 481.3/833.2 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 593.9/833.2 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 727.0/833.2 kB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  829.4/833.2 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 833.2/833.2 kB 1.8 MB/s eta 0:00:00\n",
      "Installing collected packages: torchrl\n",
      "Successfully installed torchrl-0.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchrl==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0752609b-879c-4276-a14e-dfdc880ff8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zoey\\anaconda3\\Lib\\site-packages\\torchrl\\data\\replay_buffers\\samplers.py:37: UserWarning: Failed to import torchrl C++ binaries. Some modules (eg, prioritized replay buffers) may not work with your installation. If you installed TorchRL from PyPI, please report the bug on TorchRL github. If you installed TorchRL locally and/or in development mode, check that you have all the required compiling packages.\n",
      "  warnings.warn(EXTENSION_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0329000-3ad1-4290-9c30-e75ad77e8bf4",
   "metadata": {},
   "source": [
    "Initializing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b51b8-6285-4684-bceb-4a6956181dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zoey\\anaconda3\\Lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\zoey\\anaconda3\\Lib\\site-packages\\gym\\envs\\registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3),\n",
      " 0.0,\n",
      " False,\n",
      " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zoey\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "if gym.__version__ < '0.26':\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n",
    "else:\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='human', apply_api_compatibility=True)\n",
    "\n",
    "# Action space: 0-walk right, 1-jump right\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, trunc, info = env.step(action=0)\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dc8298-c334-4aff-8fa2-a2035aa1798a",
   "metadata": {},
   "source": [
    "Using wrappers to preprocess environment data to give our agent only information it needs: making the structure of the environment a 3D array: [4, 84, 84]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609fca43-4c7c-466d-ab1f-94279f56a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "if gym.__version__ < '0.26':\n",
    "    env = FrameStack(env, num_stack=4, new_step_api=True)\n",
    "else:\n",
    "    env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38a3028-e5e6-46e6-ac39-d80a6609cc2f",
   "metadata": {},
   "source": [
    "#### Agent: Mario  \n",
    "Goals  \n",
    "- Makes optimal action policy based on current state  \n",
    "- Learns a better action policy over time\n",
    "- Q learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4100d669-0c25-400f-8048-063ab4d6977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__():\n",
    "        pass\n",
    "    def action(self, state):\n",
    "        \"\"\"Choose optimal action, given a state\"\"\"\n",
    "        pass\n",
    "    def remember(self, experience):\n",
    "        \"\"\"Add the experience to memory\"\"\"\n",
    "        pass\n",
    "    def recall(self):\n",
    "        \"\"\"Sample experience\"\"\"\n",
    "        pass\n",
    "    def learn(self):\n",
    "        \"\"\"Update online action value (Q) function\"\"\"\n",
    "        pass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797b22e8-a9c5-416b-a3f7-ffcace617899",
   "metadata": {},
   "source": [
    "#### Imitation Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d457539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c07c50-db04-4b36-9a53-4ba3c1ff2f03",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pygame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# calling and playing the game to learn\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      5\u001b[0m expert_data \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pygame'"
     ]
    }
   ],
   "source": [
    "# calling and playing the game to learn\n",
    "import pygame\n",
    "from tqdm import tqdm\n",
    "\n",
    "expert_data = []\n",
    "\n",
    "def play_and_record(env, num_episodes=5):\n",
    "    print(\"Starting expert data collection. Use arrow keys + A/Z for jump.\")\n",
    "    actions = {\n",
    "        pygame.K_RIGHT: 0,  \n",
    "        pygame.K_z: 1       \n",
    "    }\n",
    "\n",
    "    pygame.init()\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        print(f\"Episode {episode + 1}\")\n",
    "        while not done:\n",
    "            env.render()\n",
    "\n",
    "            action = 0\n",
    "            keys = pygame.key.get_pressed()\n",
    "            if keys[pygame.K_z]:  \n",
    "                action = 1\n",
    "            elif keys[pygame.K_RIGHT]:\n",
    "                action = 0\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            expert_data.append((state, action))  \n",
    "            state = next_state\n",
    "\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    return\n",
    "\n",
    "    pygame.quit()\n",
    "    return expert_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cdbe03-73d3-493c-a22f-08f22d96efb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting expert data collection. Use arrow keys + A/Z for jump.\n",
      "Episode 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bkuzn\\anaconda3\\envs\\myenv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:272: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2\n",
      "Episode 3\n",
      "Episode 4\n",
      "Episode 5\n"
     ]
    }
   ],
   "source": [
    "demo = play_and_record(env)\n",
    "torch.save(demo, \"expert_demo.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c5bfd0-1332-46f4-8308-1b51a13ba876",
   "metadata": {},
   "source": [
    "##### Defining the dataset for loading expert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035aad52-6125-4086-9681-ed293f788e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42) \n",
    "\n",
    "class ExpertDataset(Dataset):\n",
    "    def __init__(self, expert_data):\n",
    "        self.states = [torch.tensor(np.array(s), dtype=torch.float32) / 255.0 for s, _ in expert_data]\n",
    "        self.actions = [a for _, a in expert_data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.states[idx], self.actions[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a923f24a-d725-4cc0-8f53-f250dd8c1398",
   "metadata": {},
   "source": [
    "##### Define the CNN model for behavior cloning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933eccc3-1950-41a8-b364-a330c2a943f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImitationCNN(nn.Module):\n",
    "    def __init__(self, num_actions=2):\n",
    "        super(ImitationCNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),  # (84-8)/4+1 = 20\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), # (20-4)/2+1 = 9\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), # (9-3)/1+1 = 7\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47affda9-84f3-435e-b6b0-2c387ad42dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load expert data in state action pairs\n",
    "expert_data = torch.load(\"expert_demo.pt\")  \n",
    "\n",
    "dataset = ExpertDataset(expert_data)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c44be1-2f1f-4711-aad8-f6a60dbbb41b",
   "metadata": {},
   "source": [
    "##### Initialize model, loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d256824e-becf-483d-81eb-be6a60d6147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ImitationCNN(num_actions=2).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b987089c-30a9-42ff-a723-a651a9c21ef8",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21414109-5946-4d6c-ad35-7eebc03a5e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bkuzn\\AppData\\Local\\Temp\\ipykernel_37932\\2038417918.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  actions = torch.tensor(actions).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [1/7], Loss: 0.6859\n",
      "Epoch [1] completed. Average Loss: 0.6506\n",
      "Epoch [2/10], Batch [1/7], Loss: 0.6055\n",
      "Epoch [2] completed. Average Loss: 0.5668\n",
      "Epoch [3/10], Batch [1/7], Loss: 0.5081\n",
      "Epoch [3] completed. Average Loss: 0.4528\n",
      "Epoch [4/10], Batch [1/7], Loss: 0.3720\n",
      "Epoch [4] completed. Average Loss: 0.3043\n",
      "Epoch [5/10], Batch [1/7], Loss: 0.2135\n",
      "Epoch [5] completed. Average Loss: 0.1536\n",
      "Epoch [6/10], Batch [1/7], Loss: 0.0857\n",
      "Epoch [6] completed. Average Loss: 0.0551\n",
      "Epoch [7/10], Batch [1/7], Loss: 0.0260\n",
      "Epoch [7] completed. Average Loss: 0.0164\n",
      "Epoch [8/10], Batch [1/7], Loss: 0.0080\n",
      "Epoch [8] completed. Average Loss: 0.0054\n",
      "Epoch [9/10], Batch [1/7], Loss: 0.0032\n",
      "Epoch [9] completed. Average Loss: 0.0024\n",
      "Epoch [10/10], Batch [1/7], Loss: 0.0016\n",
      "Epoch [10] completed. Average Loss: 0.0013\n",
      "Model saved as imitation_mario.pth\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (states, actions) in enumerate(dataloader):\n",
    "        states = states.to(device)              \n",
    "        actions = torch.tensor(actions).to(device)  \n",
    "\n",
    "        # forward pass\n",
    "        logits = model(states)  \n",
    "        \n",
    "        # compute loss\n",
    "        loss = loss_fn(logits, actions)\n",
    "\n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}] completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), \"imitation_mario.pth\")\n",
    "print(\"Model saved as imitation_mario.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292b4263-3813-4575-a981-25b98de39be2",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbc5fce-4810-472b-b445-ba00106ae969",
   "metadata": {},
   "source": [
    "##### Evaluating accuracy of imitation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413016e6-d568-407c-b20f-e740ec89d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "expert_data = torch.load(\"expert_demo.pt\")\n",
    "train_data, test_data = train_test_split(expert_data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = ExpertDataset(train_data)\n",
    "test_dataset = ExpertDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12689de1-8638-40e5-b984-dc09d70a7702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, dataset, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    loader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for states, labels in loader:\n",
    "            states = states.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(states)\n",
    "            preds = logits.argmax(dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Model Accuracy on Held-Out Data: {acc*100:.2f}%\")\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ee7c50-d7fd-4db2-8cbc-62be8b3c3069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on Held-Out Data: 100.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(model, test_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6a1ff3-ff17-45b1-aab4-a13c19473f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_imitation_model(model, env, episodes=5, device=\"cpu\", render=False):\n",
    "    model.eval()\n",
    "    rewards = []\n",
    "    distances = []\n",
    "    steps_survived = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            # preprocess state\n",
    "            state_tensor = torch.tensor(np.array(state), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            state_tensor = state_tensor / 255.0\n",
    "\n",
    "            # predicting the action\n",
    "            with torch.no_grad():\n",
    "                logits = model(state_tensor)\n",
    "                action = logits.argmax(dim=1).item()\n",
    "\n",
    "            state, reward, done, _, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            step_count += 1\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        distances.append(info.get('x_pos', 0))\n",
    "        steps_survived.append(step_count)\n",
    "\n",
    "        print(f\"Episode {ep+1}: Reward={total_reward}, Distance={distances[-1]}, Steps={step_count}\")\n",
    "\n",
    "    print(f\"Avg Reward: {sum(rewards)/len(rewards):.2f}\")\n",
    "    print(f\"Avg Distance: {sum(distances)/len(distances):.2f}\")\n",
    "    print(f\"Avg Steps Survived: {sum(steps_survived)/len(steps_survived):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b18835-863e-498b-aef5-c493bb1e5b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward=231.0, Distance=296, Steps=40\n",
      "Episode 2: Reward=231.0, Distance=296, Steps=40\n",
      "Episode 3: Reward=231.0, Distance=296, Steps=40\n",
      "Episode 4: Reward=231.0, Distance=296, Steps=40\n",
      "Episode 5: Reward=231.0, Distance=296, Steps=40\n",
      "Avg Reward: 231.00\n",
      "Avg Distance: 296.00\n",
      "Avg Steps Survived: 40.00\n"
     ]
    }
   ],
   "source": [
    "evaluate_imitation_model(model, env, episodes=5, device=device, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f175b5-b6c1-4faa-aedd-67639ba5e68a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e869db6",
   "metadata": {},
   "source": [
    "### Q LEARNING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fc8ea9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 62\u001b[0m\n\u001b[0;32m     60\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n\u001b[1;32m---> 62\u001b[0m     state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     63\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "class MarioDQN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(MarioDQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "replay_buffer = deque(maxlen=10000)\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "min_epsilon = 0.05\n",
    "decay = 0.995\n",
    "\n",
    "policy_net = MarioDQN(num_actions=2).to(device)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def choose_action(state):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, 1)\n",
    "    state = torch.tensor(np.array(state), dtype=torch.float32).unsqueeze(0).to(device) / 255.0\n",
    "    with torch.no_grad():\n",
    "        return policy_net(state).argmax().item()\n",
    "\n",
    "def train_dqn():\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "    batch = random.sample(replay_buffer, batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "    states = torch.stack([torch.tensor(s, dtype=torch.float32) for s in states]).to(device) / 255.0\n",
    "    next_states = torch.stack([torch.tensor(s, dtype=torch.float32) for s in next_states]).to(device) / 255.0\n",
    "    actions = torch.tensor(actions).to(device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "    q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
    "    with torch.no_grad():\n",
    "        max_next_q = policy_net(next_states).max(1)[0]\n",
    "        target = rewards + gamma * max_next_q * (1 - dones)\n",
    "\n",
    "    loss = loss_fn(q_values, target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "episodes = 500\n",
    "epsilon = None\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = choose_action(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "        train_dqn()\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    epsilon = max(min_epsilon, epsilon * decay)\n",
    "    print(f\"Episode {ep+1}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c6ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioDoubleDQN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(MarioDoubleDQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 4, 84, 84)  # assume 84x84 input; change accordingly\n",
    "            conv_out = self.conv(dummy_input)\n",
    "            self.conv_output_size = conv_out.shape[1]\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.conv_output_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float() / 255.0  \n",
    "        features = self.conv(x)\n",
    "        return self.head(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec0cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_net = MarioDoubleDQN(num_actions).to(device) # implemented target network to make the Q values more stable\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_update_freq = 1000  # update target network every 1000 steps\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=6.25e-5, eps=1.5e-4)\n",
    "loss_fn = nn.SmoothL1Loss() # use Huber loss instead of MSE maybe?? might result in more stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812b250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_double_dqn(step_count):\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "\n",
    "    batch = random.sample(replay_buffer, batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    \n",
    "    states = torch.stack([torch.tensor(s, dtype=torch.float32) for s in states]).to(device)\n",
    "    next_states = torch.stack([torch.tensor(s, dtype=torch.float32) for s in next_states]).to(device)\n",
    "    actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "    \n",
    "    current_q = policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_actions = policy_net(next_states).argmax(1)\n",
    "        next_q = target_net(next_states).gather(1, next_actions.unsqueeze(1))\n",
    "        target = rewards.unsqueeze(1) + gamma * next_q * (1 - dones.unsqueeze(1))\n",
    "    \n",
    "    loss = loss_fn(current_q, target)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 10)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step_count % target_update_freq == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4988f171",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 500\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 30000\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)\n",
    "total_steps = 0\n",
    "best_reward = -float('inf')\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    episode_loss = 0\n",
    "    step_count = 0\n",
    "    \n",
    "    while not done:\n",
    "        total_steps += 1\n",
    "        step_count += 1\n",
    "\n",
    "        epsilon = epsilon_by_frame(total_steps)\n",
    "        \n",
    "        action = choose_action(state, epsilon) # select action\n",
    "        next_state, reward, done, _, _ = env.step(action) # execute action\n",
    "        replay_buffer.append((state, action, reward, next_state, done)) # store transition\n",
    "\n",
    "        loss = train_double_dqn(total_steps) # train\n",
    "        if loss is not None:\n",
    "            episode_loss += loss\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    if total_steps % target_update_freq == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    avg_loss = episode_loss / step_count if step_count > 0 else 0\n",
    "    print(f\"Episode {ep+1}, Steps: {step_count}, Total Reward: {total_reward:.1f}, \"\n",
    "          f\"Avg Loss: {avg_loss:.4f}, Epsilon: {epsilon:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
