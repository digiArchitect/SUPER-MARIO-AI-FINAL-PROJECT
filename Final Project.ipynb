{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c96624a7-0092-4fb6-a237-093bfba745a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.26.4 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00dcbc3a-9183-42d0-9df5-a1201e1497f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.1.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (2.1.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torchvision==0.16.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (0.16.0)\n",
      "Requirement already satisfied: torchaudio==2.1.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch==2.1.0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch==2.1.0) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch==2.1.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch==2.1.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch==2.1.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch==2.1.0) (2023.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torchvision==0.16.0) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torchvision==0.16.0) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torchvision==0.16.0) (10.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from jinja2->torch==2.1.0) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from requests->torchvision==0.16.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from requests->torchvision==0.16.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from requests->torchvision==0.16.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from requests->torchvision==0.16.0) (2024.8.30)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from sympy->torch==2.1.0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fc48f29-804e-41bc-9592-32fdd0bb66c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym-super-mario-bros==7.4.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (7.4.0)\n",
      "Requirement already satisfied: nes-py>=8.1.4 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from gym-super-mario-bros==7.4.0) (8.2.1)\n",
      "Requirement already satisfied: gym>=0.17.2 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.26.4)\n",
      "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.5.21)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (4.65.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (2.2.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.0.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from tqdm>=4.48.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym-super-mario-bros==7.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff207443-6cb3-4122-94c7-2c6e9f38df0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensordict==0.3.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: torch>=2.1.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from tensordict==0.3.0) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from tensordict==0.3.0) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from tensordict==0.3.0) (2.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->tensordict==0.3.0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->tensordict==0.3.0) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->tensordict==0.3.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->tensordict==0.3.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->tensordict==0.3.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->tensordict==0.3.0) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.1.0->tensordict==0.3.0) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from sympy->torch>=2.1.0->tensordict==0.3.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensordict==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df8b48f9-ca7d-4319-a257-d265db7c7875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchrl==0.3.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: torch>=2.1.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torchrl==0.3.0) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torchrl==0.3.0) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torchrl==0.3.0) (23.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torchrl==0.3.0) (2.2.1)\n",
      "Requirement already satisfied: tensordict>=0.3.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torchrl==0.3.0) (0.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->torchrl==0.3.0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->torchrl==0.3.0) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->torchrl==0.3.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->torchrl==0.3.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->torchrl==0.3.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->torchrl==0.3.0) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.1.0->torchrl==0.3.0) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from sympy->torch>=2.1.0->torchrl==0.3.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchrl==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0752609b-879c-4276-a14e-dfdc880ff8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zoey\\anaconda3\\Lib\\site-packages\\torchrl\\data\\replay_buffers\\samplers.py:37: UserWarning: Failed to import torchrl C++ binaries. Some modules (eg, prioritized replay buffers) may not work with your installation. If you installed TorchRL from PyPI, please report the bug on TorchRL github. If you installed TorchRL locally and/or in development mode, check that you have all the required compiling packages.\n",
      "  warnings.warn(EXTENSION_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0329000-3ad1-4290-9c30-e75ad77e8bf4",
   "metadata": {},
   "source": [
    "Initializing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad6b51b8-6285-4684-bceb-4a6956181dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zoey\\anaconda3\\Lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\zoey\\anaconda3\\Lib\\site-packages\\gym\\envs\\registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3),\n",
      " 0.0,\n",
      " False,\n",
      " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zoey\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "if gym.__version__ < '0.26':\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n",
    "else:\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='human', apply_api_compatibility=True)\n",
    "\n",
    "# Action space: 0-walk right, 1-jump right\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, trunc, info = env.step(action=0)\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dc8298-c334-4aff-8fa2-a2035aa1798a",
   "metadata": {},
   "source": [
    "Using wrappers to preprocess environment data to give our agent only information it needs: making the structure of the environment a 3D array: [4, 84, 84]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "609fca43-4c7c-466d-ab1f-94279f56a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "if gym.__version__ < '0.26':\n",
    "    env = FrameStack(env, num_stack=4, new_step_api=True)\n",
    "else:\n",
    "    env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38a3028-e5e6-46e6-ac39-d80a6609cc2f",
   "metadata": {},
   "source": [
    "#### Agent: Mario  \n",
    "Goals  \n",
    "- Makes optimal action policy based on current state  \n",
    "- Learns a better action policy over time\n",
    "- Q learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4100d669-0c25-400f-8048-063ab4d6977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__():\n",
    "        pass\n",
    "    def action(self, state):\n",
    "        \"\"\"Choose optimal action, given a state\"\"\"\n",
    "        pass\n",
    "    def remember(self, experience):\n",
    "        \"\"\"Add the experience to memory\"\"\"\n",
    "        pass\n",
    "    def recall(self):\n",
    "        \"\"\"Sample experience\"\"\"\n",
    "        pass\n",
    "    def learn(self):\n",
    "        \"\"\"Update online action value (Q) function\"\"\"\n",
    "        pass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797b22e8-a9c5-416b-a3f7-ffcace617899",
   "metadata": {},
   "source": [
    "#### Imitation Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d457539e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygame in c:\\users\\zoey\\anaconda3\\lib\\site-packages (2.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3c07c50-db04-4b36-9a53-4ba3c1ff2f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling and playing the game to learn\n",
    "import pygame\n",
    "from tqdm import tqdm\n",
    "\n",
    "expert_data = []\n",
    "\n",
    "def play_and_record(env, num_episodes=5):\n",
    "    print(\"Starting expert data collection. Use arrow keys + A/Z for jump.\")\n",
    "    actions = {\n",
    "        pygame.K_RIGHT: 0,  \n",
    "        pygame.K_z: 1       \n",
    "    }\n",
    "\n",
    "    pygame.init()\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        print(f\"Episode {episode + 1}\")\n",
    "        while not done:\n",
    "            env.render()\n",
    "\n",
    "            action = 0\n",
    "            keys = pygame.key.get_pressed()\n",
    "            if keys[pygame.K_z]:  \n",
    "                action = 1\n",
    "            elif keys[pygame.K_RIGHT]:\n",
    "                action = 0\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            expert_data.append((state, action))  \n",
    "            state = next_state\n",
    "\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    return\n",
    "\n",
    "    pygame.quit()\n",
    "    return expert_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56cdbe03-73d3-493c-a22f-08f22d96efb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting expert data collection. Use arrow keys + A/Z for jump.\n",
      "Episode 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zoey\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:272: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2\n",
      "Episode 3\n",
      "Episode 4\n",
      "Episode 5\n"
     ]
    }
   ],
   "source": [
    "demo = play_and_record(env)\n",
    "torch.save(demo, \"expert_demo.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c5bfd0-1332-46f4-8308-1b51a13ba876",
   "metadata": {},
   "source": [
    "##### Defining the dataset for loading expert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "035aad52-6125-4086-9681-ed293f788e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42) \n",
    "\n",
    "class ExpertDataset(Dataset):\n",
    "    def __init__(self, expert_data):\n",
    "        self.states = [torch.tensor(np.array(s), dtype=torch.float32) / 255.0 for s, _ in expert_data]\n",
    "        self.actions = [a for _, a in expert_data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.states[idx], self.actions[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a923f24a-d725-4cc0-8f53-f250dd8c1398",
   "metadata": {},
   "source": [
    "##### Define the CNN model for behavior cloning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "933eccc3-1950-41a8-b364-a330c2a943f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImitationCNN(nn.Module):\n",
    "    def __init__(self, num_actions=2):\n",
    "        super(ImitationCNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),  # (84-8)/4+1 = 20\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), # (20-4)/2+1 = 9\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), # (9-3)/1+1 = 7\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47affda9-84f3-435e-b6b0-2c387ad42dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load expert data in state action pairs\n",
    "expert_data = torch.load(\"expert_demo.pt\")  \n",
    "\n",
    "dataset = ExpertDataset(expert_data)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c44be1-2f1f-4711-aad8-f6a60dbbb41b",
   "metadata": {},
   "source": [
    "##### Initialize model, loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d256824e-becf-483d-81eb-be6a60d6147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ImitationCNN(num_actions=2).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b987089c-30a9-42ff-a723-a651a9c21ef8",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21414109-5946-4d6c-ad35-7eebc03a5e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zoey\\AppData\\Local\\Temp\\ipykernel_15612\\2038417918.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  actions = torch.tensor(actions).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [1/7], Loss: 0.6859\n",
      "Epoch [1] completed. Average Loss: 0.6506\n",
      "Epoch [2/10], Batch [1/7], Loss: 0.6055\n",
      "Epoch [2] completed. Average Loss: 0.5667\n",
      "Epoch [3/10], Batch [1/7], Loss: 0.5078\n",
      "Epoch [3] completed. Average Loss: 0.4521\n",
      "Epoch [4/10], Batch [1/7], Loss: 0.3706\n",
      "Epoch [4] completed. Average Loss: 0.3023\n",
      "Epoch [5/10], Batch [1/7], Loss: 0.2111\n",
      "Epoch [5] completed. Average Loss: 0.1517\n",
      "Epoch [6/10], Batch [1/7], Loss: 0.0845\n",
      "Epoch [6] completed. Average Loss: 0.0543\n",
      "Epoch [7/10], Batch [1/7], Loss: 0.0257\n",
      "Epoch [7] completed. Average Loss: 0.0162\n",
      "Epoch [8/10], Batch [1/7], Loss: 0.0079\n",
      "Epoch [8] completed. Average Loss: 0.0053\n",
      "Epoch [9/10], Batch [1/7], Loss: 0.0030\n",
      "Epoch [9] completed. Average Loss: 0.0022\n",
      "Epoch [10/10], Batch [1/7], Loss: 0.0015\n",
      "Epoch [10] completed. Average Loss: 0.0012\n",
      "Model saved as imitation_mario.pth\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (states, actions) in enumerate(dataloader):\n",
    "        states = states.to(device)              \n",
    "        actions = torch.tensor(actions).to(device)  \n",
    "\n",
    "        # forward pass\n",
    "        logits = model(states)  \n",
    "        \n",
    "        # compute loss\n",
    "        loss = loss_fn(logits, actions)\n",
    "\n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}] completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), \"imitation_mario.pth\")\n",
    "print(\"Model saved as imitation_mario.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292b4263-3813-4575-a981-25b98de39be2",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbc5fce-4810-472b-b445-ba00106ae969",
   "metadata": {},
   "source": [
    "##### Evaluating accuracy of imitation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "413016e6-d568-407c-b20f-e740ec89d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "expert_data = torch.load(\"expert_demo.pt\")\n",
    "train_data, test_data = train_test_split(expert_data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = ExpertDataset(train_data)\n",
    "test_dataset = ExpertDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12689de1-8638-40e5-b984-dc09d70a7702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, dataset, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    loader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for states, labels in loader:\n",
    "            states = states.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(states)\n",
    "            preds = logits.argmax(dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Model Accuracy on Held-Out Data: {acc*100:.2f}%\")\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20ee7c50-d7fd-4db2-8cbc-62be8b3c3069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on Held-Out Data: 100.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(model, test_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb6a1ff3-ff17-45b1-aab4-a13c19473f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_imitation_model(model, env, episodes=5, device=\"cpu\", render=False):\n",
    "    model.eval()\n",
    "    rewards = []\n",
    "    distances = []\n",
    "    steps_survived = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            # preprocess state\n",
    "            state_tensor = torch.tensor(np.array(state), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            state_tensor = state_tensor / 255.0\n",
    "\n",
    "            # predicting the action\n",
    "            with torch.no_grad():\n",
    "                logits = model(state_tensor)\n",
    "                action = logits.argmax(dim=1).item()\n",
    "\n",
    "            state, reward, done, _, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            step_count += 1\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        distances.append(info.get('x_pos', 0))\n",
    "        steps_survived.append(step_count)\n",
    "\n",
    "        print(f\"Episode {ep+1}: Reward={total_reward}, Distance={distances[-1]}, Steps={step_count}\")\n",
    "\n",
    "    print(f\"Avg Reward: {sum(rewards)/len(rewards):.2f}\")\n",
    "    print(f\"Avg Distance: {sum(distances)/len(distances):.2f}\")\n",
    "    print(f\"Avg Steps Survived: {sum(steps_survived)/len(steps_survived):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03b18835-863e-498b-aef5-c493bb1e5b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward=231.0, Distance=296, Steps=40\n",
      "Episode 2: Reward=231.0, Distance=296, Steps=40\n",
      "Episode 3: Reward=231.0, Distance=296, Steps=40\n",
      "Episode 4: Reward=231.0, Distance=296, Steps=40\n",
      "Episode 5: Reward=231.0, Distance=296, Steps=40\n",
      "Avg Reward: 231.00\n",
      "Avg Distance: 296.00\n",
      "Avg Steps Survived: 40.00\n"
     ]
    }
   ],
   "source": [
    "evaluate_imitation_model(model, env, episodes=5, device=device, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f175b5-b6c1-4faa-aedd-67639ba5e68a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e869db6",
   "metadata": {},
   "source": [
    "### Q LEARNING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fc8ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MarioDQN(nn.Module):\n",
    "#     def __init__(self, num_actions):\n",
    "#         super(MarioDQN, self).__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(64 * 7 * 7, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, num_actions)\n",
    "#         )\n",
    "\n",
    "# simplified version of the DQN model to save time\n",
    "class MarioDQN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(4, 16, kernel_size=8, stride=4), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 9 * 9, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "replay_buffer = deque(maxlen=10000)\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "min_epsilon = 0.05\n",
    "decay = 0.995\n",
    "update_interval = 4\n",
    "\n",
    "policy_net = MarioDQN(num_actions=2).to(device)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def choose_action(state):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, 1)\n",
    "    state = torch.tensor(np.array(state), dtype=torch.float32).unsqueeze(0).to(device) / 255.0\n",
    "    with torch.no_grad():\n",
    "        return policy_net(state).argmax().item()\n",
    "\n",
    "def train_dqn():\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "    batch = random.sample(replay_buffer, batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "    states = torch.stack([torch.tensor(s, dtype=torch.float32) for s in states]).to(device) / 255.0\n",
    "    next_states = torch.stack([torch.tensor(s, dtype=torch.float32) for s in next_states]).to(device) / 255.0\n",
    "    actions = torch.tensor(actions).to(device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "    q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
    "    with torch.no_grad():\n",
    "        max_next_q = policy_net(next_states).max(1)[0]\n",
    "        target = rewards + gamma * max_next_q * (1 - dones)\n",
    "\n",
    "    loss = loss_fn(q_values, target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "episodes = 1000\n",
    "print_every = 10 # print every 10 episodes\n",
    "total_steps = 0\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = choose_action(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        state = np.array(state)\n",
    "        next_state = np.array(next_state)\n",
    "            \n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "        total_steps += 1\n",
    "        if total_steps % update_interval == 0:\n",
    "            train_dqn()\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    epsilon = max(min_epsilon, epsilon * decay)\n",
    "    if (ep + 1) % print_every == 0:\n",
    "        print(f\"Episode {ep+1}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c6ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioDoubleDQN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(MarioDoubleDQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 4, 84, 84)  # assume 84x84 input; change accordingly\n",
    "            conv_out = self.conv(dummy_input)\n",
    "            self.conv_output_size = conv_out.shape[1]\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.conv_output_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float() / 255.0  \n",
    "        features = self.conv(x)\n",
    "        return self.head(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec0cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_net = MarioDoubleDQN(num_actions).to(device) # add target network to make the Q values more stable\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_update_freq = 1000  # update target network every 1000 steps\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=6.25e-5, eps=1.5e-4)\n",
    "loss_fn = nn.SmoothL1Loss() # use Huber loss instead of MSE maybe?? might result in more stability, unsure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812b250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_double_dqn(step_count):\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "\n",
    "    batch = random.sample(replay_buffer, batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    \n",
    "    states = torch.stack([torch.tensor(s, dtype=torch.float32) for s in states]).to(device)\n",
    "    next_states = torch.stack([torch.tensor(s, dtype=torch.float32) for s in next_states]).to(device)\n",
    "    actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "    \n",
    "    current_q = policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_actions = policy_net(next_states).argmax(1)\n",
    "        next_q = target_net(next_states).gather(1, next_actions.unsqueeze(1))\n",
    "        target = rewards.unsqueeze(1) + gamma * next_q * (1 - dones.unsqueeze(1))\n",
    "    \n",
    "    loss = loss_fn(current_q, target)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 10)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step_count % target_update_freq == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4988f171",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 500\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 30000\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)\n",
    "total_steps = 0\n",
    "best_reward = -float('inf')\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    episode_loss = 0\n",
    "    step_count = 0\n",
    "    \n",
    "    while not done:\n",
    "        total_steps += 1\n",
    "        step_count += 1\n",
    "\n",
    "        epsilon = epsilon_by_frame(total_steps)\n",
    "        \n",
    "        action = choose_action(state, epsilon) # select action\n",
    "        next_state, reward, done, _, _ = env.step(action) # execute action\n",
    "        replay_buffer.append((state, action, reward, next_state, done)) # store transition\n",
    "\n",
    "        loss = train_double_dqn(total_steps) # train\n",
    "        if loss is not None:\n",
    "            episode_loss += loss\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    if total_steps % target_update_freq == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    avg_loss = episode_loss / step_count if step_count > 0 else 0\n",
    "    print(f\"Episode {ep+1}, Steps: {step_count}, Total Reward: {total_reward:.1f}, \"\n",
    "          f\"Avg Loss: {avg_loss:.4f}, Epsilon: {epsilon:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
