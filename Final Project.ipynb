{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c96624a7-0092-4fb6-a237-093bfba745a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.26.4 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00dcbc3a-9183-42d0-9df5-a1201e1497f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.1.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: torchvision==0.16.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (0.16.0)\n",
      "Requirement already satisfied: torchaudio==2.1.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch==2.1.0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch==2.1.0) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch==2.1.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch==2.1.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch==2.1.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch==2.1.0) (2023.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torchvision==0.16.0) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torchvision==0.16.0) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torchvision==0.16.0) (10.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from jinja2->torch==2.1.0) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from requests->torchvision==0.16.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from requests->torchvision==0.16.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from requests->torchvision==0.16.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from requests->torchvision==0.16.0) (2024.8.30)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from sympy->torch==2.1.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc48f29-804e-41bc-9592-32fdd0bb66c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym-super-mario-bros==7.4.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (7.4.0)\n",
      "Requirement already satisfied: nes-py>=8.1.4 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from gym-super-mario-bros==7.4.0) (8.2.1)\n",
      "Requirement already satisfied: gym>=0.17.2 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.26.4)\n",
      "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.5.21)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (4.65.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (2.2.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.0.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from tqdm>=4.48.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "pip install gym-super-mario-bros==7.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff207443-6cb3-4122-94c7-2c6e9f38df0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensordict==0.3.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: torch>=2.1.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from tensordict==0.3.0) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from tensordict==0.3.0) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from tensordict==0.3.0) (2.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->tensordict==0.3.0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->tensordict==0.3.0) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->tensordict==0.3.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->tensordict==0.3.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->tensordict==0.3.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->tensordict==0.3.0) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.1.0->tensordict==0.3.0) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from sympy->torch>=2.1.0->tensordict==0.3.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensordict==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b48f9-ca7d-4319-a257-d265db7c7875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchrl==0.3.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: torch>=2.1.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torchrl==0.3.0) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torchrl==0.3.0) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torchrl==0.3.0) (23.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torchrl==0.3.0) (2.2.1)\n",
      "Requirement already satisfied: tensordict>=0.3.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torchrl==0.3.0) (0.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->torchrl==0.3.0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->torchrl==0.3.0) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->torchrl==0.3.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->torchrl==0.3.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->torchrl==0.3.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from torch>=2.1.0->torchrl==0.3.0) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.1.0->torchrl==0.3.0) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\zoey\\anaconda3\\lib\\site-packages (from sympy->torch>=2.1.0->torchrl==0.3.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchrl==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0752609b-879c-4276-a14e-dfdc880ff8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zoey\\anaconda3\\Lib\\site-packages\\torchrl\\data\\replay_buffers\\samplers.py:37: UserWarning: Failed to import torchrl C++ binaries. Some modules (eg, prioritized replay buffers) may not work with your installation. If you installed TorchRL from PyPI, please report the bug on TorchRL github. If you installed TorchRL locally and/or in development mode, check that you have all the required compiling packages.\n",
      "  warnings.warn(EXTENSION_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0329000-3ad1-4290-9c30-e75ad77e8bf4",
   "metadata": {},
   "source": [
    "Initializing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b51b8-6285-4684-bceb-4a6956181dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zoey\\anaconda3\\Lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\zoey\\anaconda3\\Lib\\site-packages\\gym\\envs\\registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3),\n",
      " 0.0,\n",
      " False,\n",
      " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zoey\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "if gym.__version__ < '0.26':\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n",
    "else:\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='human', apply_api_compatibility=True)\n",
    "\n",
    "# Action space: 0-walk right, 1-jump right\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, trunc, info = env.step(action=0)\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dc8298-c334-4aff-8fa2-a2035aa1798a",
   "metadata": {},
   "source": [
    "Using wrappers to preprocess environment data to give our agent only information it needs: making the structure of the environment a 3D array: [4, 84, 84]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609fca43-4c7c-466d-ab1f-94279f56a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "if gym.__version__ < '0.26':\n",
    "    env = FrameStack(env, num_stack=4, new_step_api=True)\n",
    "else:\n",
    "    env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38a3028-e5e6-46e6-ac39-d80a6609cc2f",
   "metadata": {},
   "source": [
    "#### Agent: Mario  \n",
    "Goals  \n",
    "- Makes optimal action policy based on current state  \n",
    "- Learns a better action policy over time\n",
    "- Q learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4100d669-0c25-400f-8048-063ab4d6977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__():\n",
    "        pass\n",
    "    def action(self, state):\n",
    "        \"\"\"Choose optimal action, given a state\"\"\"\n",
    "        pass\n",
    "    def remember(self, experience):\n",
    "        \"\"\"Add the experience to memory\"\"\"\n",
    "        pass\n",
    "    def recall(self):\n",
    "        \"\"\"Sample experience\"\"\"\n",
    "        pass\n",
    "    def learn(self):\n",
    "        \"\"\"Update online action value (Q) function\"\"\"\n",
    "        pass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797b22e8-a9c5-416b-a3f7-ffcace617899",
   "metadata": {},
   "source": [
    "#### Imitation Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d457539e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygame in c:\\users\\zoey\\anaconda3\\lib\\site-packages (2.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c07c50-db04-4b36-9a53-4ba3c1ff2f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling and playing the game to learn\n",
    "import pygame\n",
    "from tqdm import tqdm\n",
    "\n",
    "expert_data = []\n",
    "\n",
    "def play_and_record(env, num_episodes=5):\n",
    "    print(\"Starting expert data collection. Use arrow keys + A/Z for jump.\")\n",
    "    actions = {\n",
    "        pygame.K_RIGHT: 0,  \n",
    "        pygame.K_z: 1       \n",
    "    }\n",
    "\n",
    "    pygame.init()\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        print(f\"Episode {episode + 1}\")\n",
    "        while not done:\n",
    "            env.render()\n",
    "\n",
    "            action = 0\n",
    "            keys = pygame.key.get_pressed()\n",
    "            if keys[pygame.K_z]:  \n",
    "                action = 1\n",
    "            elif keys[pygame.K_RIGHT]:\n",
    "                action = 0\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            expert_data.append((state, action))  \n",
    "            state = next_state\n",
    "\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    return\n",
    "\n",
    "    pygame.quit()\n",
    "    return expert_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cdbe03-73d3-493c-a22f-08f22d96efb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting expert data collection. Use arrow keys + A/Z for jump.\n",
      "Episode 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zoey\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:272: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2\n",
      "Episode 3\n",
      "Episode 4\n",
      "Episode 5\n"
     ]
    }
   ],
   "source": [
    "demo = play_and_record(env)\n",
    "torch.save(demo, \"expert_demo.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c5bfd0-1332-46f4-8308-1b51a13ba876",
   "metadata": {},
   "source": [
    "##### Defining the dataset for loading expert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035aad52-6125-4086-9681-ed293f788e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42) \n",
    "\n",
    "class ExpertDataset(Dataset):\n",
    "    def __init__(self, expert_data):\n",
    "        self.states = [torch.tensor(np.array(s), dtype=torch.float32) / 255.0 for s, _ in expert_data]\n",
    "        self.actions = [a for _, a in expert_data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.states[idx], self.actions[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a923f24a-d725-4cc0-8f53-f250dd8c1398",
   "metadata": {},
   "source": [
    "##### Define the CNN model for behavior cloning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933eccc3-1950-41a8-b364-a330c2a943f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImitationCNN(nn.Module):\n",
    "    def __init__(self, num_actions=2):\n",
    "        super(ImitationCNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),  # (84-8)/4+1 = 20\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), # (20-4)/2+1 = 9\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), # (9-3)/1+1 = 7\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47affda9-84f3-435e-b6b0-2c387ad42dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load expert data in state action pairs\n",
    "expert_data = torch.load(\"expert_demo.pt\")  \n",
    "\n",
    "dataset = ExpertDataset(expert_data)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c44be1-2f1f-4711-aad8-f6a60dbbb41b",
   "metadata": {},
   "source": [
    "##### Initialize model, loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d256824e-becf-483d-81eb-be6a60d6147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ImitationCNN(num_actions=2).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b987089c-30a9-42ff-a723-a651a9c21ef8",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21414109-5946-4d6c-ad35-7eebc03a5e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zoey\\AppData\\Local\\Temp\\ipykernel_23768\\2038417918.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  actions = torch.tensor(actions).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [1/7], Loss: 0.6859\n",
      "Epoch [1] completed. Average Loss: 0.6506\n",
      "Epoch [2/10], Batch [1/7], Loss: 0.6055\n",
      "Epoch [2] completed. Average Loss: 0.5667\n",
      "Epoch [3/10], Batch [1/7], Loss: 0.5078\n",
      "Epoch [3] completed. Average Loss: 0.4521\n",
      "Epoch [4/10], Batch [1/7], Loss: 0.3706\n",
      "Epoch [4] completed. Average Loss: 0.3023\n",
      "Epoch [5/10], Batch [1/7], Loss: 0.2111\n",
      "Epoch [5] completed. Average Loss: 0.1517\n",
      "Epoch [6/10], Batch [1/7], Loss: 0.0845\n",
      "Epoch [6] completed. Average Loss: 0.0543\n",
      "Epoch [7/10], Batch [1/7], Loss: 0.0257\n",
      "Epoch [7] completed. Average Loss: 0.0162\n",
      "Epoch [8/10], Batch [1/7], Loss: 0.0079\n",
      "Epoch [8] completed. Average Loss: 0.0053\n",
      "Epoch [9/10], Batch [1/7], Loss: 0.0030\n",
      "Epoch [9] completed. Average Loss: 0.0022\n",
      "Epoch [10/10], Batch [1/7], Loss: 0.0015\n",
      "Epoch [10] completed. Average Loss: 0.0012\n",
      "Model saved as imitation_mario.pth\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (states, actions) in enumerate(dataloader):\n",
    "        states = states.to(device)              \n",
    "        actions = torch.tensor(actions).to(device)  \n",
    "\n",
    "        # forward pass\n",
    "        logits = model(states)  \n",
    "        \n",
    "        # compute loss\n",
    "        loss = loss_fn(logits, actions)\n",
    "\n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}] completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), \"imitation_mario.pth\")\n",
    "print(\"Model saved as imitation_mario.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292b4263-3813-4575-a981-25b98de39be2",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbc5fce-4810-472b-b445-ba00106ae969",
   "metadata": {},
   "source": [
    "##### Evaluating accuracy of imitation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413016e6-d568-407c-b20f-e740ec89d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "expert_data = torch.load(\"expert_demo.pt\")\n",
    "train_data, test_data = train_test_split(expert_data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = ExpertDataset(train_data)\n",
    "test_dataset = ExpertDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12689de1-8638-40e5-b984-dc09d70a7702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, dataset, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    loader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for states, labels in loader:\n",
    "            states = states.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(states)\n",
    "            preds = logits.argmax(dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Model Accuracy on Held-Out Data: {acc*100:.2f}%\")\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ee7c50-d7fd-4db2-8cbc-62be8b3c3069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on Held-Out Data: 100.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(model, test_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6a1ff3-ff17-45b1-aab4-a13c19473f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_imitation_model(model, env, episodes=5, device=\"cpu\", render=False):\n",
    "    model.eval()\n",
    "    rewards = []\n",
    "    distances = []\n",
    "    steps_survived = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            # preprocess state\n",
    "            state_tensor = torch.tensor(np.array(state), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            state_tensor = state_tensor / 255.0\n",
    "\n",
    "            # predicting the action\n",
    "            with torch.no_grad():\n",
    "                logits = model(state_tensor)\n",
    "                action = logits.argmax(dim=1).item()\n",
    "\n",
    "            state, reward, done, _, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            step_count += 1\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        distances.append(info.get('x_pos', 0))\n",
    "        steps_survived.append(step_count)\n",
    "\n",
    "        print(f\"Episode {ep+1}: Reward={total_reward}, Distance={distances[-1]}, Steps={step_count}\")\n",
    "\n",
    "    print(f\"Avg Reward: {sum(rewards)/len(rewards):.2f}\")\n",
    "    print(f\"Avg Distance: {sum(distances)/len(distances):.2f}\")\n",
    "    print(f\"Avg Steps Survived: {sum(steps_survived)/len(steps_survived):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b18835-863e-498b-aef5-c493bb1e5b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward=231.0, Distance=296, Steps=40\n",
      "Episode 2: Reward=231.0, Distance=296, Steps=40\n",
      "Episode 3: Reward=231.0, Distance=296, Steps=40\n",
      "Episode 4: Reward=231.0, Distance=296, Steps=40\n",
      "Episode 5: Reward=231.0, Distance=296, Steps=40\n",
      "Avg Reward: 231.00\n",
      "Avg Distance: 296.00\n",
      "Avg Steps Survived: 40.00\n"
     ]
    }
   ],
   "source": [
    "evaluate_imitation_model(model, env, episodes=5, device=device, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f175b5-b6c1-4faa-aedd-67639ba5e68a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e869db6",
   "metadata": {},
   "source": [
    "### Q LEARNING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f2f5cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m CHECKPOINT_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmario_checkpoint.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m SAVE_INTERVAL \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# save every 100 episodes\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMarioDQN\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_actions):\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "CHECKPOINT_PATH = \"mario_checkpoint.pth\"\n",
    "SAVE_INTERVAL = 100  # save every 100 episodes\n",
    "\n",
    "class MarioDQN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(4, 16, kernel_size=8, stride=4), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 9 * 9, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "replay_buffer = deque(maxlen=10000)\n",
    "policy_net = MarioDQN(num_actions=2).to(device)\n",
    "target_net = MarioDQN(num_actions=2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "min_epsilon = 0.05\n",
    "decay = 0.995\n",
    "update_interval = 4\n",
    "target_update_interval = 1000\n",
    "\n",
    "def choose_action(state):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, 1)\n",
    "    state = torch.tensor(np.array(state), dtype=torch.float32).unsqueeze(0).to(device) / 255.0\n",
    "    with torch.no_grad():\n",
    "        return policy_net(state).argmax().item()\n",
    "\n",
    "def train_dqn():\n",
    "    global total_steps, replay_buffer\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "    \n",
    "    batch = random.sample(replay_buffer, batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "    states = torch.stack([torch.tensor(s, dtype=torch.float32) for s in states]).to(device) / 255.0\n",
    "    next_states = torch.stack([torch.tensor(s, dtype=torch.float32) for s in next_states]).to(device) / 255.0\n",
    "    actions = torch.tensor(actions).to(device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "    q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        next_actions = policy_net(next_states).argmax(dim=1)\n",
    "        next_q_values = target_net(next_states)\n",
    "        max_next_q = next_q_values.gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "        target = rewards + gamma * max_next_q * (1 - dones)\n",
    "\n",
    "    loss = loss_fn(q_values, target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e3c228",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m total_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      5\u001b[0m start_episode \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(CHECKPOINT_PATH): \u001b[38;5;66;03m# used to save experience to avoid having to start over every run\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading checkpoint from last saved episode...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(CHECKPOINT_PATH)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "print_every = 10\n",
    "total_steps = 0\n",
    "\n",
    "start_episode = 0\n",
    "if os.path.exists(CHECKPOINT_PATH): # used to save experience to avoid having to start over every run\n",
    "    print(\"Loading checkpoint from last saved episode...\")\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "    policy_net.load_state_dict(checkpoint['model'])\n",
    "    target_net.load_state_dict(checkpoint['target_model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    replay_buffer = pickle.loads(checkpoint['replay_buffer'])\n",
    "    epsilon = checkpoint['epsilon']\n",
    "    total_steps = checkpoint['total_steps']\n",
    "    start_episode = checkpoint['episode'] + 1\n",
    "\n",
    "for ep in range(start_episode, episodes):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = choose_action(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        replay_buffer.append((\n",
    "            np.array(state), \n",
    "            action, \n",
    "            reward, \n",
    "            np.array(next_state), \n",
    "            done\n",
    "        ))\n",
    "        \n",
    "        total_steps += 1\n",
    " \n",
    "        if total_steps % update_interval == 0:\n",
    "            train_dqn()\n",
    "            if total_steps % target_update_interval == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    epsilon = max(min_epsilon, epsilon * decay)\n",
    "    \n",
    "    if (ep + 1) % print_every == 0:\n",
    "        print(f\"Episode {ep+1}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "    \n",
    "    if (ep + 1) % SAVE_INTERVAL == 0: # save model every 100 episodes\n",
    "        checkpoint = {\n",
    "            'model': policy_net.state_dict(),\n",
    "            'target_model': target_net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'replay_buffer': pickle.dumps(replay_buffer),\n",
    "            'epsilon': epsilon,\n",
    "            'total_steps': total_steps,\n",
    "            'episode': ep\n",
    "        }\n",
    "        torch.save(checkpoint, CHECKPOINT_PATH)\n",
    "        print(f\"Saved checkpoint at episode {ep+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fc8ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10, Total Reward: 231.00, Epsilon: 0.951\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 84\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     83\u001b[0m     action \u001b[38;5;241m=\u001b[39m choose_action(state)\n\u001b[1;32m---> 84\u001b[0m     next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     86\u001b[0m     state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(state)\n\u001b[0;32m     87\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(next_state)\n",
      "File \u001b[1;32mc:\\Users\\zoey\\anaconda3\\Lib\\site-packages\\gym\\wrappers\\frame_stack.py:173\u001b[0m, in \u001b[0;36mFrameStack.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m    165\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, appending the observation to the frame buffer.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03m        Stacked observations, reward, terminated, truncated, and information from the environment\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m.\u001b[39mappend(observation)\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(\u001b[38;5;28;01mNone\u001b[39;00m), reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\zoey\\anaconda3\\Lib\\site-packages\\gym\\core.py:385\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[39;00m\n\u001b[0;32m    384\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m--> 385\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "Cell \u001b[1;32mIn[9], line 53\u001b[0m, in \u001b[0;36mResizeObservation.observation\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobservation\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation):\n\u001b[0;32m     50\u001b[0m     transforms \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mCompose(\n\u001b[0;32m     51\u001b[0m         [T\u001b[38;5;241m.\u001b[39mResize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, antialias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), T\u001b[38;5;241m.\u001b[39mNormalize(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)]\n\u001b[0;32m     52\u001b[0m     )\n\u001b[1;32m---> 53\u001b[0m     observation \u001b[38;5;241m=\u001b[39m transforms(observation)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation\n",
      "File \u001b[1;32mc:\\Users\\zoey\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\zoey\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zoey\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zoey\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mresize(img, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mantialias)\n",
      "File \u001b[1;32mc:\\Users\\zoey\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\functional.py:492\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    489\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39mpil_interpolation)\n\u001b[1;32m--> 492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[1;32mc:\\Users\\zoey\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:462\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, antialias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m antialias \u001b[38;5;129;01mand\u001b[39;00m interpolation \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;66;03m# We manually set it to False to avoid an error downstream in interpolate()\u001b[39;00m\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;66;03m# This behaviour is documented: the parameter is irrelevant for modes\u001b[39;00m\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;66;03m# that are not bilinear or bicubic. We used to raise an error here, but\u001b[39;00m\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;66;03m# now we don't as True is the default.\u001b[39;00m\n\u001b[0;32m    460\u001b[0m     antialias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 462\u001b[0m img, need_cast, need_squeeze, out_dtype \u001b[38;5;241m=\u001b[39m _cast_squeeze_in(img, [torch\u001b[38;5;241m.\u001b[39mfloat32, torch\u001b[38;5;241m.\u001b[39mfloat64])\n\u001b[0;32m    464\u001b[0m \u001b[38;5;66;03m# Define align_corners to avoid warnings\u001b[39;00m\n\u001b[0;32m    465\u001b[0m align_corners \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m interpolation \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zoey\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:520\u001b[0m, in \u001b[0;36m_cast_squeeze_in\u001b[1;34m(img, req_dtypes)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;66;03m# make image NCHW\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m--> 520\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    521\u001b[0m     need_squeeze \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    523\u001b[0m out_dtype \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mdtype\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# class MarioDQN(nn.Module):\n",
    "#     def __init__(self, num_actions):\n",
    "#         super(MarioDQN, self).__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(64 * 7 * 7, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, num_actions)\n",
    "#         )\n",
    "\n",
    "# simplified version of the DQN model to save time\n",
    "class MarioDQN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(4, 16, kernel_size=8, stride=4), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 9 * 9, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "replay_buffer = deque(maxlen=10000)\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "min_epsilon = 0.05\n",
    "decay = 0.995\n",
    "update_interval = 4\n",
    "\n",
    "policy_net = MarioDQN(num_actions=2).to(device)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def choose_action(state):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, 1)\n",
    "    state = torch.tensor(np.array(state), dtype=torch.float32).unsqueeze(0).to(device) / 255.0\n",
    "    with torch.no_grad():\n",
    "        return policy_net(state).argmax().item()\n",
    "\n",
    "def train_dqn():\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "    batch = random.sample(replay_buffer, batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "    states = torch.stack([torch.tensor(s, dtype=torch.float32) for s in states]).to(device) / 255.0\n",
    "    next_states = torch.stack([torch.tensor(s, dtype=torch.float32) for s in next_states]).to(device) / 255.0\n",
    "    actions = torch.tensor(actions).to(device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "    q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
    "    with torch.no_grad():\n",
    "        max_next_q = policy_net(next_states).max(1)[0]\n",
    "        target = rewards + gamma * max_next_q * (1 - dones)\n",
    "\n",
    "    loss = loss_fn(q_values, target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "episodes = 1000\n",
    "print_every = 10 # print every 10 episodes\n",
    "total_steps = 0\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = choose_action(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        state = np.array(state)\n",
    "        next_state = np.array(next_state)\n",
    "            \n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "        total_steps += 1\n",
    "        if total_steps % update_interval == 0:\n",
    "            train_dqn()\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    epsilon = max(min_epsilon, epsilon * decay)\n",
    "    if (ep + 1) % print_every == 0:\n",
    "        print(f\"Episode {ep+1}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
